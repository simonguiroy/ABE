{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ABE_tensorflow.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCgcsfFBv7zH"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.stats\n",
        "import scipy.signal\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "#Initialize the experiment : get list of saved model checkpoints and other parameters"
      ],
      "metadata": {
        "id": "ZfddSkGTxXSs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_analysis_trials = self.learn_config.num_analysis_episodes\n",
        "analyze_every = self.learn_config.analyze_every\n",
        "\n",
        "logging.info('Performing analysis in representation space w.r.t the %s split using %d episodes...',\n",
        "              split, num_analysis_trials)\n",
        "logging.info('Evaluating generalization on the %s split using %d episodes...',\n",
        "              split, num_eval_trials)\n",
        "\n",
        "# Get directory where model checkpoints are saved\n",
        "eval_checkpoints_dir = self.learner_config.checkpoint_for_eval.split('model_')[0]\n",
        "# List all checkpoint filenames, in the form model_{iteration}.ckpt\n",
        "eval_checkpoints = [f for f in listdir(eval_checkpoints_dir) if (isfile(join(eval_checkpoints_dir, f)) and 'model_' in f and '.ckpt.index' in f)]\n",
        "eval_checkpoints.sort()\n",
        "for i in range(len(eval_checkpoints)):\n",
        "  eval_checkpoints[i] = eval_checkpoints[i].strip('.index')\n",
        "eval_checkpoints.sort()\n",
        "# Now we have a list that is sorted alphabetically, but we need the numeric order of checkpoints\n",
        "# i.e.: alphabetic order: model_1000, model_1500, model_500, numerical order: model_500, model_1000, model_1500\n",
        "# Getting the list of saved iterations, in numerical order\n",
        "for i in range(len(eval_checkpoints)):\n",
        "  eval_checkpoints[i] = int(eval_checkpoints[i].strip('.cktp').strip('model_'))\n",
        "eval_checkpoints.sort()\n",
        "# Creating the list of filenames with the numerical order of saved iterations\n",
        "for i in range(len(eval_checkpoints)):\n",
        "  eval_checkpoints[i] = 'model_' + str(eval_checkpoints[i]) + '.ckpt'\n",
        "\n",
        "# Analyze every x checkpoint\n",
        "analysis_indices = list(range(0, len(eval_checkpoints), analyze_every))\n",
        "eval_checkpoints = [eval_checkpoints[i] for i in analysis_indices]\n"
      ],
      "metadata": {
        "id": "FjAY30OkvvMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check if the experiment is actually already completed"
      ],
      "metadata": {
        "id": "MNQX2DAZxm5S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if the experiment is actually already completed\n",
        "if os.path.isfile(join(self.summary_dir, 'completed.npy')):\n",
        "  sys.exit()\n",
        "\n",
        "# Check if experiment has already started\n",
        "elif os.path.isfile(join(self.summary_dir, 'activation_moments_trajectory.npy')):\n",
        "  exp_dict = np.load(join(self.summary_dir, 'activation_moments_trajectory.npy'), allow_pickle=True)\n",
        "  exp_dict = exp_dict.item()\n",
        "\n",
        "  m1_models_list = exp_dict['m1_models_list']\n",
        "  m2_models_list = exp_dict['m2_models_list']\n",
        "  m3_models_list = exp_dict['m3_models_list']\n",
        "  m4_models_list = exp_dict['m4_models_list']\n",
        "  analyzed_models_list = exp_dict['analyzed_models_list']\n",
        "  episode_list = exp_dict['episode_list']\n",
        "\n",
        "  # Resuming the experiment after the last analyzed checkpoint\n",
        "  last_analyzed_model = exp_dict['analyzed_models_list'][-1]\n",
        "  last_analyzed_model_index = eval_checkpoints.index(last_analyzed_model)\n",
        "  eval_checkpoints = eval_checkpoints[last_analyzed_model_index+1:]\n",
        "\n",
        "# If experiment has not started yet\n",
        "else:\n",
        "  # Initializing the experiment data lists\n",
        "  m1_models_list = []\n",
        "  m2_models_list = []\n",
        "  m3_models_list = []\n",
        "  m4_models_list = []\n",
        "  analyzed_models_list = []\n",
        "\n",
        "  # Getting a static list of the support sets of all episodes in the test split.\n",
        "  # This way, the same episodes will be used for all checkpoints of the model, to monitor the evolution of the metric.\n",
        "  episodes = self.learners['test'].data.train_images\n",
        "  episode_list = []\n",
        "  for analysis_trial_num in range(num_analysis_trials):\n",
        "    episode_tensors = self.sess.run(episodes)\n",
        "    episode_list.append(episode_tensors)"
      ],
      "metadata": {
        "id": "zqiY9UTHwni4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Example of how to get the intermediate activations in a neural network"
      ],
      "metadata": {
        "id": "Yz5003Rs5JmR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _four_layer_convnet(inputs,\n",
        "                        scope,\n",
        "                        reuse=tf.AUTO_REUSE,\n",
        "                        params=None,\n",
        "                        moments=None,\n",
        "                        depth_multiplier=1.0,\n",
        "                        backprop_through_moments=True,\n",
        "                        use_bounded_activation=False,\n",
        "                        keep_spatial_dims=False):\n",
        "  \"\"\"A four-layer-convnet architecture.\"\"\"\n",
        "  layer = tf.stop_gradient(inputs)\n",
        "  model_params_keys, model_params_vars = [], []\n",
        "  moments_keys, moments_vars = [], []\n",
        "  intermediate_activations = [] # ADD CODE HERE\n",
        "\n",
        "  with tf.variable_scope(scope, reuse=reuse):\n",
        "    for i in range(4):\n",
        "      with tf.variable_scope('layer_{}'.format(i), reuse=reuse):\n",
        "        depth = int(64 * depth_multiplier)\n",
        "        layer, conv_bn_params, conv_bn_moments = conv_bn(\n",
        "            layer, [3, 3],\n",
        "            depth,\n",
        "            stride=1,\n",
        "            params=params,\n",
        "            moments=moments,\n",
        "            backprop_through_moments=backprop_through_moments)\n",
        "        model_params_keys.extend(conv_bn_params.keys())\n",
        "        model_params_vars.extend(conv_bn_params.values())\n",
        "        moments_keys.extend(conv_bn_moments.keys())\n",
        "        moments_vars.extend(conv_bn_moments.values())\n",
        "\n",
        "      if use_bounded_activation:\n",
        "        layer = tf.nn.relu6(layer)\n",
        "      else:\n",
        "        layer = tf.nn.relu(layer)\n",
        "      layer = tf.layers.max_pooling2d(layer, [2, 2], 2)\n",
        "      intermediate_activations.append(layer) # ADD CODE HERE\n",
        "      logging.info('Output of block %d: %s', i, layer.shape)\n",
        "\n",
        "    model_params = collections.OrderedDict(\n",
        "        zip(model_params_keys, model_params_vars))\n",
        "    moments = collections.OrderedDict(zip(moments_keys, moments_vars))\n",
        "    if not keep_spatial_dims:\n",
        "      layer = tf.layers.flatten(layer)\n",
        "    return_dict = {\n",
        "        'embeddings': layer,\n",
        "        'params': model_params,\n",
        "        'moments': moments,\n",
        "        'intermediate_activations': intermediate_activations # ADD CODE HERE\n",
        "    }\n",
        "\n",
        "    return return_dict"
      ],
      "metadata": {
        "id": "LFykbRA45NpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Functions to compute the neural activation trajectory"
      ],
      "metadata": {
        "id": "v-ALPS1ox1bF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_m1_hat(h_vectors_):\n",
        "  \"\"\"\n",
        "  Computes the first raw moment (sample mean) of first-order moments.\n",
        "  First-order moments are the (sample) means of the individual activation features.\n",
        "  For a matrix H \\in R^{N \\times d} of N activation vectors h, where h \\in R^d :\n",
        "  m1_hat = \\frac{1}{d} \\sum_{j=1}^{d} \\frac{1}{N} \\sum{i=1}^{N} H_{i,j}\n",
        "  :param h_vectors_: the activation vectors\n",
        "  :return:\n",
        "  \"\"\"\n",
        "  H = tf.layers.flatten(h_vectors_)\n",
        "  N = H.shape[0]\n",
        "  d = H.shape[1]\n",
        "\n",
        "  # First-order moment of the activation vectors : mean vector\n",
        "  m1 = tf.math.reduce_sum(H, axis=0)\n",
        "  m1 = tf.math.divide(m1, tf.compat.v1.to_float(N, name='ToFloat'))\n",
        "\n",
        "  # m1_hat : first aggregated moment - first-order moment across feature dimensions of the mean activation vector\n",
        "  m1_hat = tf.math.reduce_sum(m1, axis=0)\n",
        "  m1_hat = tf.math.divide(m1_hat, tf.compat.v1.to_float(d, name='ToFloat'))\n",
        "\n",
        "  return m1_hat\n",
        "\n",
        "def compute_m2_hat(h_vectors_):\n",
        "  \"\"\"\n",
        "  Computes the second raw moment (sample variance aroud zero) of first-order moments.\n",
        "  First-order moments are the (sample) means of the individual activation features.\n",
        "  For a matrix H \\in R^{N \\times d} of N activation vectors h, where h \\in R^d :\n",
        "  m2_hat = \\frac{1}{d} \\sum_{j=1}^{d} ( \\frac{1}{N} \\sum{i=1}^{N} H_{i,j} )^2\n",
        "  :param h_vectors_: the activation vectors\n",
        "  :return:\n",
        "  \"\"\"\n",
        "  H = tf.layers.flatten(h_vectors_)\n",
        "  N = H.shape[0]\n",
        "  d = H.shape[1]\n",
        "\n",
        "  # First-order moment of the activation vectors : mean vector\n",
        "  m1 = tf.math.reduce_sum(H, axis=0)\n",
        "  m1 = tf.math.divide(m1, tf.compat.v1.to_float(N, name='ToFloat'))\n",
        "\n",
        "  # m2_hat : second aggregated moment - second-order moment across feature dimensions of the mean activation vector\n",
        "  m2_hat = tf.math.square(m1)\n",
        "  m2_hat = tf.math.reduce_sum(m2_hat, axis=0)\n",
        "  m2_hat = tf.math.divide(m2_hat, tf.compat.v1.to_float(d, name='ToFloat'))\n",
        "\n",
        "  return m2_hat\n",
        "\n",
        "def compute_m3_hat(h_vectors_):\n",
        "  \"\"\"\n",
        "  Computes the first raw moment (sample mean) of the diagonal elements of the matrix of second-order moments (auto-correlation matrix).\n",
        "  Diagonal second-order moments are the (sample) variances of the individual activation features.\n",
        "  For a matrix H \\in R^{N \\times d} of N activation vectors h, where h \\in R^d :\n",
        "  m3_hat = \\frac{1}{d} \\sum_{j=1}^{d} \\frac{1}{N} \\sum{i=1}^{N} H_{i,j}^2\n",
        "  :param h_vectors_: the activation vectors\n",
        "  :return:\n",
        "  \"\"\"\n",
        "  H = tf.layers.flatten(h_vectors_)\n",
        "  N = H.shape[0]\n",
        "  d = H.shape[1]\n",
        "\n",
        "  # m3_hat : third aggregated moment - first-order moment across feature dimensions of the diagonal elements of the auto-correlation matrix\n",
        "  m3_hat = tf.math.square(H)\n",
        "  m3_hat = tf.math.reduce_sum(m3_hat, axis=0)\n",
        "  m3_hat = tf.math.divide(m3_hat, tf.compat.v1.to_float(N, name='ToFloat'))\n",
        "  m3_hat = tf.math.reduce_sum(m3_hat, axis=0)\n",
        "  m3_hat = tf.math.divide(m3_hat, tf.compat.v1.to_float(d, name='ToFloat'))\n",
        "\n",
        "  return m3_hat\n",
        "\n",
        "def compute_m4_hat(h_vectors_, max_block_size=500):\n",
        "  \"\"\"\n",
        "  Computes the first raw moment (sample mean) of the non-diagonal elements of the matrix of second-order moments.\n",
        "  Non-diagonal second-order moments are the (sample) covariances of the individual activation features.\n",
        "  For a matrix H \\in R^{N \\times d} of N activation vectors h, where h \\in R^d :\n",
        "  m4_hat = \\frac{1}{d^2 - d} \\sum_{k=1}^{d} \\sum_{j=1, j \\neq k}^{d} \\frac{1}{N} \\sum{i=1}^{N} H_{i,j} \\time H_{i,k}\n",
        "  :param h_vectors_: the activation vectors\n",
        "  :param max_block_size: size of smaller blocks to break the matrix multiplication for the covariances\n",
        "  :return:\n",
        "  \"\"\"\n",
        "  H = tf.layers.flatten(h_vectors_)\n",
        "  N = H.shape[0]\n",
        "  d = H.shape[1]\n",
        "\n",
        "  m4_hat = 0.0\n",
        "  num_blocks = d.value / max_block_size\n",
        "  for i in range(np.int(np.floor(num_blocks))):\n",
        "    B = H[:, i * max_block_size:(i + 1) * max_block_size]\n",
        "    m4_hat = m4_hat + tf.math.reduce_sum(tf.matmul(tf.transpose(B), B), axis=None)\n",
        "  if num_blocks % 1 > 0:\n",
        "    B = H[:, np.int(np.floor(num_blocks)) * max_block_size:]\n",
        "    m4_hat = m4_hat + tf.math.reduce_sum(tf.matmul(tf.transpose(B), B), axis=None)\n",
        "  m4_hat = m4_hat - tf.math.reduce_sum(tf.math.square(H), axis=None)\n",
        "  m4_hat = tf.math.divide(m4_hat, tf.compat.v1.to_float(N * (d * (d - 1)), name='ToFloat'))\n",
        "\n",
        "  return m4_hat"
      ],
      "metadata": {
        "id": "bzktAZgTu4-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compute the neural activation trajectory"
      ],
      "metadata": {
        "id": "_8O9k8EOxQ2I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For each checkpoint, running the analysis on each episode\n",
        "for eval_checkpoint in eval_checkpoints:\n",
        "  # Restoring the model parameters from the saved checkpoint\n",
        "  self.saver.restore(self.sess, join(eval_checkpoints_dir, eval_checkpoint))\n",
        "  m1_tasks_list = []\n",
        "  m2_tasks_list = []\n",
        "  m3_tasks_list = []\n",
        "  m4_tasks_list = []\n",
        "  with tf.Session() as sess:\n",
        "    # For each episode, computing the average inner product between the representations of the support examples\n",
        "    for analysis_trial_num in range(num_analysis_trials):\n",
        "      # Getting the activations of all the layers of the feature network\n",
        "      m1_layers_list = []\n",
        "      m2_layers_list = []\n",
        "      m3_layers_list = []\n",
        "      m4_layers_list = []\n",
        "\n",
        "      intermediate_activations = self.embedding_fn(episode_list[analysis_trial_num], is_training=False)['intermediate_activations']\n",
        "      for layer_id in range(len(intermediate_activations)):\n",
        "        m1 = compute_m1_hat(intermediate_activations[layer_id])\n",
        "        m2 = compute_m2_hat(intermediate_activations[layer_id])\n",
        "        m3 = compute_m3_hat(intermediate_activations[layer_id])\n",
        "        m4 = compute_m4_hat(intermediate_activations[layer_id])\n",
        "\n",
        "        m1_layers_list.append(m1)\n",
        "        m2_layers_list.append(m2)\n",
        "        m3_layers_list.append(m3)\n",
        "        m4_layers_list.append(m4)\n",
        "\n",
        "      m1_tasks_list.append(m1_layers_list)\n",
        "      m2_tasks_list.append(m2_layers_list)\n",
        "      m3_tasks_list.append(m3_layers_list)\n",
        "      m4_tasks_list.append(m4_layers_list)\n",
        "\n",
        "    m1_tasks_list_values, m2_tasks_list_values, m3_tasks_list_values, m4_tasks_list_values = self.sess.run([m1_tasks_list,\n",
        "                                                                                                            m2_tasks_list,\n",
        "                                                                                                            m3_tasks_list,\n",
        "                                                                                                            m4_tasks_list])\n",
        "  m1_models_list.append(m1_tasks_list_values)\n",
        "  m2_models_list.append(m2_tasks_list_values)\n",
        "  m3_models_list.append(m3_tasks_list_values)\n",
        "  m4_models_list.append(m4_tasks_list_values)\n",
        "\n",
        "  analyzed_models_list.append(eval_checkpoint)\n",
        "  print('Iteration:\\t ' + str(eval_checkpoint))\n",
        "\n",
        "  np.save(join(self.summary_dir, 'activation_moments_trajectory.npy'), {'m1_models_list': m1_models_list,\n",
        "                                                                  'm2_models_list': m2_models_list,\n",
        "                                                                  'm3_models_list': m3_models_list,\n",
        "                                                                  'm4_models_list': m4_models_list,\n",
        "                                                                  'analyzed_models_list': analyzed_models_list,\n",
        "                                                                  'episode_list': episode_list})\n",
        "\n",
        "# The experiment is complete. Saving empty file \"completed.npy\" to signal that the experiment is done.\n",
        "np.save(join(self.summary_dir, 'completed.npy'), None)"
      ],
      "metadata": {
        "id": "V4HfJtJCu5EV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparing the target and source neural activation trajectories"
      ],
      "metadata": {
        "id": "flj-r-X9zNhT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utilities"
      ],
      "metadata": {
        "id": "exzDehxNzUrN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def temporal_smoothing_v3(metric, step=5):\n",
        "    if step == 0:\n",
        "        return metric\n",
        "    metric_array = metric\n",
        "    l = len(metric_array)\n",
        "    avg_metric = np.zeros(l)\n",
        "    counts = np.zeros(l)\n",
        "    \n",
        "    for i in range(-step,step+1):\n",
        "        idx_start_sum = np.max([0, i])\n",
        "        idx_end_sum = np.min([i+l-1, l-1])\n",
        "        \n",
        "        idx_start_add = l - idx_end_sum - 1\n",
        "        idx_end_add = l - idx_start_sum - 1\n",
        "        \n",
        "        avg_metric[idx_start_sum:idx_end_sum+1] += metric_array[idx_start_add:idx_end_add+1]\n",
        "        counts[idx_start_sum:idx_end_sum+1] += 1\n",
        "\n",
        "    avg_metric /= counts\n",
        "        \n",
        "    return avg_metric"
      ],
      "metadata": {
        "id": "9578BqWeyCR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_acc_train_valid(acc_train, iters_train, acc_valid, iters_valid):\n",
        "    plt.figure(figsize=(15,8))\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.title('Train Accuracy')\n",
        "    plt.plot(iters_train, acc_train, label='$Acc_{train}$')\n",
        "    ax = plt.gca()\n",
        "    ax.axvline(x=iters_train[np.argmax(acc_train)], linestyle='--', linewidth=2.0, c='C1')\n",
        "    plt.legend(fontsize=15)\n",
        "    plt.grid()\n",
        "    plt.xlabel('Training iter', fontsize=15)\n",
        "    plt.ylabel('Mean Accuracy', fontsize=15)\n",
        "    plt.xticks(fontsize=15)\n",
        "    plt.yticks(fontsize=15)\n",
        "\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.title('Valid Accuracy')\n",
        "    plt.plot(iters_valid, acc_valid, label='$Acc_{valid}$')\n",
        "    ax = plt.gca()\n",
        "    ax.axvline(x=iters_valid[np.argmax(acc_valid)], linestyle='--', linewidth=2.0, c='C1')\n",
        "    plt.legend(fontsize=15)\n",
        "    plt.grid()\n",
        "    plt.xlabel('Training iter', fontsize=15)\n",
        "    plt.ylabel('Mean Accuracy', fontsize=15)\n",
        "    plt.xticks(fontsize=15)\n",
        "    plt.yticks(fontsize=15)\n"
      ],
      "metadata": {
        "id": "oamhaJAN7JAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_acc_target(acc_target, iters_target, target_dataset):\n",
        "    plt.figure(figsize=(8,8))\n",
        "    plt.title('Target Accuracy')\n",
        "    plt.plot(iters_target, acc_target, label='$Acc_{target}$ - ' + target_dataset)\n",
        "    plt.legend(fontsize=15)\n",
        "    plt.grid()\n",
        "    plt.xlabel('Training iter', fontsize=15)\n",
        "    plt.ylabel('Mean Accuracy', fontsize=15)\n",
        "    plt.xticks(fontsize=15)\n",
        "    plt.yticks(fontsize=15)\n",
        "    ax = plt.gca()\n",
        "    ax.axvline(x=iters_target[np.argmax(acc_target)], linestyle='--', linewidth=2.0, c='C1')\n"
      ],
      "metadata": {
        "id": "VPZ0QnnG7JMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_activation_moments(moments_source, moments_source_iters, moments_target, moments_target_iters, acc_valid, layers, show_corr, task_id=0):\n",
        "    for layer in layers:\n",
        "        plt.figure(figsize=(15,6))\n",
        "        ax_list = []\n",
        "        for moment_idx in range(len(moments_target)):\n",
        "            ax = plt.subplot(1,len(moments_target),moment_idx+1)\n",
        "            ax_list.append(ax)\n",
        "            ax.title.set_text('Layer ' + str(layer+1) + ' - Moment ' + str(moment_idx+1))\n",
        "            ax.plot(moments_target_iters[0:moments_target[moment_idx].shape[0]], moments_target[moment_idx, :, task_id, layer], label='target moments')\n",
        "            ax.plot(moments_source_iters[0:moments_source[moment_idx].shape[0]], moments_source[moment_idx, :, layer], label='source moments')\n",
        "\n",
        "            \n",
        "            max_length = np.min([len(moments_target[moment_idx, :, task_id, layer]),\n",
        "                                       len(moments_source[moment_idx, :, layer])])\n",
        "            if show_corr:                \n",
        "                corr = scipy.stats.pearsonr(moments_target[moment_idx, :max_length, task_id, layer], \n",
        "                                            moments_source[moment_idx, :max_length, layer])\n",
        "\n",
        "                textstr = \"Corr= \" + str(float(\"{0:.2f}\".format(corr[0])))\n",
        "                props = dict(boxstyle='round', facecolor='white', alpha=1.0)\n",
        "                ax.text(0.5, 0.5, textstr, transform=ax.transAxes, fontsize=15,\n",
        "                        verticalalignment='top', bbox=props, zorder=1000)\n",
        "            ax.legend(loc='best')\n",
        "\n",
        "            # Compute decorrelation time\n",
        "            #peak = compute_decorrelation_time(moments_target[moment_idx, :max_length, task_id, layer], moments_source[moment_idx, :max_length, layer])\n",
        "            #ax.axvline(x=moments_target_iters[peak], linestyle='--', linewidth=2.0, c='C1')\n",
        "        for axes in ax_list: axes.grid()"
      ],
      "metadata": {
        "id": "3wQq5ov07JYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_moments(filename):\n",
        "    analysis_moments = np.load(filename, allow_pickle=True)\n",
        "    analysis_moments = analysis_moments.item()\n",
        "    iters = analysis_moments['analyzed_models_list']\n",
        "    for i in range(len(iters)):\n",
        "        iters[i] = iters[i].strip('.ckpt')\n",
        "        iters[i] = iters[i].split('_')[-1]\n",
        "        iters[i] = int(iters[i])\n",
        "    iters = np.asarray(iters)\n",
        "    m1 = np.asarray(analysis_moments['m1_models_list'])\n",
        "    m2 = np.asarray(analysis_moments['m2_models_list'])\n",
        "    m3 = np.asarray(analysis_moments['m3_models_list'])\n",
        "    m4 = np.asarray(analysis_moments['m4_models_list'])\n",
        "    moments = np.asarray([m1, m2, m3, m4])\n",
        "\n",
        "    return moments, iters\n"
      ],
      "metadata": {
        "id": "DR42V0347aQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_target_acc(exp_dir_target_acc, algo, source_dataset, run, target_dataset):\n",
        "    acc_target = None\n",
        "    iters_target = None\n",
        "    target_acc_found = False\n",
        "    analysis_target_legacy = None\n",
        "    analysis_target_new = None\n",
        "    \n",
        "    if algo == 'maml'and source_dataset == 'mini-imagenet':\n",
        "        dir_target_acc_maml_mini_imagenet = '/home/user1/PhD-E2020/FS_Model_Selection/Meta-Dataset/meta-dataset/exp/summaries/paper_exps/summaries/representation_analyses/target_accuracies/'\n",
        "        #analysis = np.load(dir_target_acc_maml_mini_imagenet + 'maml_mini-imagenet_15-extra-steps_run-' + str(run) + '_analysis_' + target_dataset + '/analysis.npy')\n",
        "        analysis = np.load(dir_target_acc_maml_mini_imagenet + 'maml-fo_5way1shot_mini-imagenet_from_scratch_convnet_target_accuracy_analysis_' + target_dataset + '/target_accuracy.npy')\n",
        "        acc_target = np.asarray(analysis.item()['mean_acc_models_list'])\n",
        "        #mean_acc = np.mean(mean_acc_models_list, axis=1)\n",
        "        iters = analysis.item()['analyzed_models_list']\n",
        "        for i in range(len(iters)):\n",
        "            iters[i] = iters[i].strip('.ckpt')\n",
        "            iters[i] = iters[i].split('_')[-1]\n",
        "            iters[i] = int(iters[i])\n",
        "        iters_target = np.asarray(iters)\n",
        "        \n",
        "    else:\n",
        "        legacy_filename = exp_dir_target_acc + algo + '_' + source_dataset + '_run-' + str(run) + '_analysis_' + target_dataset + '/analysis.npy'\n",
        "        filename = exp_dir_target_acc + 'target_accuracies/' + algo + '_' + source_dataset + '_run-' + str(run) + '_target_accuracy_analysis_' + target_dataset + '/target_accuracy.npy'\n",
        "        # First check the legacy files\n",
        "        if os.path.isfile(legacy_filename):\n",
        "            analysis_target_legacy = np.load(legacy_filename).item()\n",
        "            target_acc_found = True\n",
        "        # If legacy accuracy not found, check new files\n",
        "        if os.path.isfile(filename):\n",
        "            analysis_target_new = np.load(filename).item()\n",
        "            target_acc_found = True\n",
        "\n",
        "        if target_acc_found:\n",
        "            # Taking the experiment that ran for the longest\n",
        "            if analysis_target_new is None:\n",
        "                analysis_target = analysis_target_legacy\n",
        "            elif analysis_target_legacy is None:\n",
        "                analysis_target = analysis_target_new\n",
        "            else:\n",
        "                if len(analysis_target_legacy['analyzed_models_list']) > len(analysis_target_new['analyzed_models_list']):\n",
        "                    analysis_target = analysis_target_legacy\n",
        "                else:\n",
        "                    analysis_target = analysis_target_new\n",
        "            acc_target = np.asarray(analysis_target['mean_acc_models_list'])\n",
        "            # Formatting the iterations array\n",
        "            iters_target = analysis_target['analyzed_models_list']\n",
        "            for i in range(len(iters_target)): iters_target[i] = int(iters_target[i].strip('.ckpt').split('_')[-1])\n",
        "            iters_target = np.asarray(iters_target)\n",
        "\n",
        "        else:\n",
        "            print('Target accuracy not found !')\n",
        "    \n",
        "    return acc_target, iters_target\n"
      ],
      "metadata": {
        "id": "KJVBkmJx7bRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function for identifying the critical layer\n"
      ],
      "metadata": {
        "id": "1Ipkj0wZ0Uq1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def critical_layer_min_corr_avg_over_moments(task_idx, moments_target, moments_target_iters, moments_source, acc_valid, iters_valid):\n",
        "    min_layer_corr = np.infty\n",
        "    critical_layer_idx = None\n",
        "    for layer_idx in range(moments_target.shape[3]):\n",
        "        corr_layer = 0.0\n",
        "        # Averaging the moment correlations\n",
        "        for moment_idx in range(moments_target.shape[0]):\n",
        "            max_length = np.min([len(moments_target[moment_idx, :, task_idx, layer_idx]),\n",
        "                                 len(moments_source[moment_idx, :, layer_idx])])\n",
        "            # Note : Only condidering the trajectory between init and iter max valid, inclusively\n",
        "            limit = np.argmin(np.abs(moments_target_iters - iters_valid[np.argmax(acc_valid)])) + 1\n",
        "            limit = np.min([limit, max_length])\n",
        "            max_length = limit\n",
        "\n",
        "            corr, p_value = scipy.stats.pearsonr(\n",
        "                moments_target[moment_idx, :max_length, task_idx, layer_idx],\n",
        "                moments_source[moment_idx, :max_length, layer_idx])\n",
        "            corr_layer += corr\n",
        "        corr_layer /= moments_target.shape[0]\n",
        "        if corr_layer < min_layer_corr:\n",
        "            min_layer_corr = corr_layer\n",
        "            critical_layer_idx = layer_idx\n",
        "            \n",
        "    return critical_layer_idx"
      ],
      "metadata": {
        "id": "kQ6DwWpYzYB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Function for identifying the critical moment"
      ],
      "metadata": {
        "id": "d2y9mLVM0ZxX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def critical_moment_min_corr_before_max_valid(task_idx, critical_layer_idx, moments_target, moments_target_iters, moments_source, acc_valid, iters_valid):\n",
        "    min_moment_corr = np.infty\n",
        "    critical_moment_idx = None\n",
        "    for moment_idx in range(moments_target.shape[0]):\n",
        "        max_length = np.min([len(moments_target[moment_idx, :, task_idx, critical_layer_idx]),\n",
        "                             len(moments_source[moment_idx, :, critical_layer_idx])])\n",
        "        # Note : Only condidering the trajectory between init and iter max valid, inclusively\n",
        "        limit = np.argmin(np.abs(moments_target_iters - iters_valid[np.argmax(acc_valid)])) + 1\n",
        "        limit = np.min([limit, max_length])\n",
        "        max_length = limit\n",
        "\n",
        "        corr, p_value = scipy.stats.pearsonr(\n",
        "            moments_target[moment_idx, :max_length, task_idx, critical_layer_idx],\n",
        "            moments_source[moment_idx, :max_length, critical_layer_idx])\n",
        "        if corr < min_moment_corr:\n",
        "            min_moment_corr = corr\n",
        "            critical_moment_idx = moment_idx\n",
        "            \n",
        "        return critical_moment_idx"
      ],
      "metadata": {
        "id": "qxo4x87mzuCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function for computing stopping_time score"
      ],
      "metadata": {
        "id": "_ETqB7810h5y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_score_rightside_neg_corr_windows(task_idx, critical_layer_idx, critical_moment_idx, moments_target, moments_target_iters, moments_source, acc_valid, iters_valid, multiply_score_by_interval):\n",
        "    max_length = np.min([len(moments_target[critical_moment_idx, :, task_idx, critical_layer_idx]),\n",
        "                             len(moments_source[critical_moment_idx, :, critical_layer_idx])])\n",
        "    limit = np.argmin(np.abs(moments_target_iters - iters_valid[np.argmax(acc_valid)])) + 1\n",
        "    limit = np.min([limit, max_length])\n",
        "    max_length = limit\n",
        "\n",
        "    x = moments_target[critical_moment_idx, :max_length, task_idx, critical_layer_idx]\n",
        "    y = moments_source[critical_moment_idx, :max_length, critical_layer_idx]\n",
        "    t_0 = 0\n",
        "    max_score = - np.infty\n",
        "    scores = []\n",
        "    for t in range(len(x)):\n",
        "        interval_neg = range(t, len(x))\n",
        "        score_neg = 0.0\n",
        "        if len(interval_neg) > 1:\n",
        "            corr_neg, p_value_neg = scipy.stats.pearsonr(x[t:], y[t:])\n",
        "            if multiply_score_by_interval:\n",
        "                score_neg = - (corr_neg * len(interval_neg))\n",
        "            else:\n",
        "                score_neg = - corr_neg\n",
        "        # Only computing the score for the right region of negative correlation\n",
        "        score = score_neg\n",
        "        scores.append(score)\n",
        "    \n",
        "    return scores"
      ],
      "metadata": {
        "id": "GbvseuoszuEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Function for performing the analysis and computing the performances"
      ],
      "metadata": {
        "id": "4CLhUPhE1B-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# critical layer (CL) : Where correlation between source and target moments is lowest, averaged on the four moments\n",
        "#     difference with v1 =>>>  time window :  trajectory before iter max valid acc\n",
        "# critical moment (CM) : At CL, the moment where correlation is lowest between source and target\n",
        "#   difference with v1 =>>>  time window :  trajectory before iter max valid acc\n",
        "# critical time (CT) : At CL and CM, the time where the right region of negative correlation (-1*corr) multiplied by width, is highest.\n",
        "#                     time window :  trajectory before iter max valid acc\n",
        "# CT limit : iter max valid acc\n",
        "def negative_correlation_before_max_valid(moments_target, moments_target_iters, moments_source, acc_valid, iters_valid, acc_target, iters_target, show_stopping=False, multiply_score_by_interval=True, stop_at_first_maximum=False, force_layer_moment_inspection=None, find_layer_and_moment_by_score=False,\n",
        "                                         critical_layer_criterion='critical_layer_min_corr_avg_over_moments',\n",
        "                                         critical_moment_criterion='critical_moment_min_corr_before_max_valid',\n",
        "                                         critical_time_criterion='max_score_rightside_neg_corr_windows'):\n",
        "    perf_avg = 0.0\n",
        "    list_perf = []\n",
        "    list_iter_stopping = []\n",
        "    # NOTE : target moments arrays are of the shape : [num_moments, num_iters, num_tasks, num_layers]\n",
        "    #        source moments arrays are of the shape : [num_moments, num_iters, num_layers]\n",
        "    # Evaluating the method on every target task\n",
        "    for task_idx in range(moments_target.shape[2]):\n",
        "        layer_indices = list(range(moments_target.shape[3]))\n",
        "        moment_indices = list(range(moments_target.shape[0]))\n",
        "        \n",
        "        # 1. Identifying at which layer the distributional shift is highest\n",
        "        if critical_layer_criterion == 'critical_layer_min_corr_avg_over_moments':\n",
        "            critical_layer_idx = critical_layer_min_corr_avg_over_moments(task_idx, moments_target, moments_target_iters, moments_source, acc_valid, iters_valid)\n",
        "\n",
        "        elif critical_layer_criterion == 'max_score_rightside_neg_corr_windows':\n",
        "            max_score = - np.infty\n",
        "            indices_max_score = None\n",
        "            for layer_idx in layer_indices:\n",
        "                for moment_idx in moment_indices:\n",
        "                    scores = compute_score_rightside_neg_corr_windows(task_idx, layer_idx, moment_idx, moments_target, moments_target_iters, moments_source, acc_valid, iters_valid, multiply_score_by_interval)\n",
        "                    score = np.max(scores)\n",
        "                    if score > max_score:\n",
        "                        max_score = score\n",
        "                        indices_max_score = [layer_idx, moment_idx]\n",
        "            critical_layer_idx = indices_max_score[0]\n",
        "            \n",
        "        else:\n",
        "            print('Provided critical_layer_criterion NOT SUPPORTED !')\n",
        "            sys.exit()\n",
        "\n",
        "        # 2. Identifying which moment drives the distributional shift the most\n",
        "        if critical_moment_criterion == 'critical_moment_min_corr_before_max_valid':\n",
        "            critical_moment_idx = critical_moment_min_corr_before_max_valid(task_idx, critical_layer_idx, moments_target, moments_target_iters, moments_source, acc_valid, iters_valid)\n",
        "\n",
        "        elif critical_moment_criterion == 'max_score_rightside_neg_corr_windows':\n",
        "            if critical_layer_criterion == 'max_score_rightside_neg_corr_windows':\n",
        "                critical_moment_idx = indices_max_score[1]\n",
        "            else:\n",
        "                max_score = - np.infty\n",
        "                idx_max_score = None\n",
        "                for moment_idx in moment_indices:\n",
        "                    scores = compute_score_rightside_neg_corr_windows(task_idx, critical_layer_idx, moment_idx, moments_target, moments_target_iters, moments_source, acc_valid, iters_valid, multiply_score_by_interval)\n",
        "                    score = np.max(scores)\n",
        "                    if score > max_score:\n",
        "                        max_score = score\n",
        "                        idx_max_score = moment_idx\n",
        "                critical_moment_idx = idx_max_score\n",
        "\n",
        "        else:\n",
        "            print('Provided critical_moment_criterion NOT SUPPORTED !')\n",
        "            sys.exit()\n",
        "\n",
        "        # For debugging purposes\n",
        "        if force_layer_moment_inspection is not None:\n",
        "            critical_layer_idx = force_layer_moment_inspection[0]\n",
        "            critical_moment_idx = force_layer_moment_inspection[1]\n",
        "                \n",
        "        # 3. Identifying at which time the critical moment, at the critical layer, start to decorrelate\n",
        "        # Note : Only condidering the trajectory between init and iter max valid, inclusively\n",
        "        if critical_time_criterion == 'max_score_rightside_neg_corr_windows':\n",
        "            scores = compute_score_rightside_neg_corr_windows(task_idx, critical_layer_idx, critical_moment_idx, moments_target, moments_target_iters, moments_source, acc_valid, iters_valid, multiply_score_by_interval)\n",
        "        else:\n",
        "            print('Provided critical_time_criterion NOT SUPPORTED !')\n",
        "            sys.exit()\n",
        "\n",
        "        t_stopping = np.argmax(scores)\n",
        "        max_score = np.max(scores)\n",
        "        \n",
        "        if show_stopping and task_idx < 10:\n",
        "            max_length = np.min([len(moments_target[critical_moment_idx, :, task_idx, critical_layer_idx]),\n",
        "                                     len(moments_source[critical_moment_idx, :, critical_layer_idx])])\n",
        "            limit = np.argmin(np.abs(moments_target_iters - iters_valid[np.argmax(acc_valid)])) + 1\n",
        "            limit = np.min([limit, max_length])\n",
        "            max_length = limit\n",
        "            \n",
        "            x = moments_target[critical_moment_idx, :max_length, task_idx, critical_layer_idx]\n",
        "            y = moments_source[critical_moment_idx, :max_length, critical_layer_idx]\n",
        "            \n",
        "            if task_idx == 0:\n",
        "            #if True: #quick debug / displaying. restore line above when done !!!\n",
        "                plt.figure(figsize=(16,8))\n",
        "                plt.title('scores - target moments - source moments')\n",
        "                plt.xticks(fontsize=20)\n",
        "            plt.subplot(1,3,1)\n",
        "            plt.plot(moments_target_iters[:max_length], scores, label=(critical_layer_idx, critical_moment_idx, t_stopping, task_idx))\n",
        "            plt.scatter(moments_target_iters[t_stopping], max_score, s=50)\n",
        "            plt.legend()\n",
        "            plt.grid()\n",
        "            plt.subplot(1,3,2)\n",
        "            plt.plot(moments_target_iters[:max_length], x, label=task_idx)\n",
        "            plt.legend()\n",
        "            plt.grid()\n",
        "            plt.subplot(1,3,3)\n",
        "            plt.plot(moments_target_iters[:max_length], y, label=task_idx)\n",
        "            plt.legend()\n",
        "            plt.grid()\n",
        "        # Our method stops before or at the maximum validation, not after\n",
        "        iter_stopping = np.min([moments_target_iters[t_stopping], iters_valid[np.argmax(acc_valid)]])\n",
        "\n",
        "        # 4. Computing the generalization performance when using the method on the current target task\n",
        "        #perf_task = np.mean(acc_target, axis=1)[np.argmin(np.abs(iters_target - iter_stopping))]\n",
        "        perf_task = acc_target[np.argmin(np.abs(iters_target - iter_stopping))]\n",
        "        perf_avg += perf_task\n",
        "        list_perf.append(perf_task)\n",
        "        list_iter_stopping.append(iter_stopping)\n",
        "\n",
        "    # Averaging the performance of our method on the current target dataset\n",
        "    perf_avg /= moments_target.shape[2]\n",
        "    # Computing the performance of the validation baseline\n",
        "    iter_max_valid_acc = iters_valid[np.argmax(acc_valid)]\n",
        "    #perf_valid = np.mean(acc_target, axis=1)[np.argmin(np.abs(iters_target - iter_max_valid_acc))]\n",
        "    perf_valid = acc_target[np.argmin(np.abs(iters_target - iter_max_valid_acc))]\n",
        "\n",
        "    # Printing the performance\n",
        "    print('TARGET : ' + target_dataset)\n",
        "    print('our method : ' + str(perf_avg) + ' valid baseline : ' + str(perf_valid))\n",
        "    \n",
        "    return perf_avg, perf_valid, critical_layer_idx, critical_moment_idx, list_perf, list_iter_stopping"
      ],
      "metadata": {
        "id": "XFyWKgmLzuGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Perform early-stopping"
      ],
      "metadata": {
        "id": "9NQDSraS1m2Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Directories containing raw data\n",
        "exp_dir_activation_trajectory = '/home/user1/PhD-A2021/Neural_Computation/exps/baselines/BACKUP_activation_moments_trajectory_3/'\n",
        "exp_dir_target_acc = '/home/user1/PhD-E2020/FS_Model_Selection/Meta-Dataset/meta-dataset/exp/summaries/paper_exps/summaries/representation_analyses/'\n",
        "exp_dir_valid_acc = '/home/user1/PhD-E2020/FS_Model_Selection/Meta-Dataset/meta-dataset/exp/summaries/paper_exps/valid_acc/'\n",
        "exp_dir_train_acc = '/home/user1/PhD-E2020/FS_Model_Selection/Meta-Dataset/meta-dataset/exp/summaries/paper_exps/train_acc/'\n",
        "\n",
        "# Fields for the experiments\n",
        "algos=['maml', 'prototypical', 'matching']\n",
        "source_datasets=['cu_birds', 'dtd', 'aircraft', 'omniglot', 'vgg_flower', 'imagenet', 'mini-imagenet', 'quickdraw']\n",
        "target_datasets=['cu_birds', 'dtd', 'aircraft', 'omniglot', 'vgg_flower', 'ilsvrc_2012', 'mini_imagenet', 'quickdraw', 'traffic_sign']\n",
        "\n",
        "training_runs_dict = {'maml' : {'cu_birds': 4, 'dtd': 3, 'aircraft': 2, 'omniglot': 2, 'vgg_flower': 4, 'imagenet': 1, 'mini-imagenet': 1, 'quickdraw': 5},\n",
        "             'prototypical': {'cu_birds': 1, 'dtd': 3, 'aircraft': 4, 'omniglot': 2, 'vgg_flower': 5, 'imagenet': 1, 'mini-imagenet': 1, 'quickdraw': 1},\n",
        "             'matching': {'cu_birds': 1, 'dtd': 4, 'aircraft': 1, 'omniglot': 1, 'vgg_flower': 1, 'imagenet': 2, 'mini-imagenet': 1, 'quickdraw': 1}\n",
        "                    }\n",
        "layers = [0,1,2,3]\n",
        "\n",
        "# If insecting a single experiment, i.e. (algo, source dataset, target dataset)\n",
        "inspect_single_exp = False\n",
        "if inspect_single_exp:\n",
        "    algos = ['matching']\n",
        "    source_datasets = ['quickdraw']\n",
        "    target_datasets = ['omniglot']\n",
        "    force_layer_moment_inspection = None\n",
        "    #force_layer_moment_inspection = [3,0]\n",
        "\n",
        "force_layer_moment_inspection = None\n",
        "    \n",
        "show_plots = False\n",
        "# Options for data analysis\n",
        "plot_accuracy = show_plots\n",
        "plot_moments = show_plots\n",
        "show_stopping = show_plots\n",
        "smooth_acc_train = True\n",
        "smooth_acc_valid = True\n",
        "smooth_acc_target = True\n",
        "smooth_moments = False\n",
        "multiply_score_by_interval = True\n",
        "stop_at_first_maximum = False\n",
        "num_smoothing_steps = 3\n",
        "show_corr = True\n",
        "task_id = 0  # inspecting a single target task (debugging)\n",
        "\n",
        "stopping_criterion = 'negative_correlation_before_max_valid'\n",
        "critical_layer_criterion='max_score_rightside_neg_corr_windows'\n",
        "critical_moment_criterion='max_score_rightside_neg_corr_windows'\n",
        "critical_time_criterion='max_score_rightside_neg_corr_windows'\n",
        "\n",
        "\n",
        "analysis_arguments = {}\n",
        "analysis_arguments['smooth_acc_train'] = smooth_acc_train\n",
        "analysis_arguments['smooth_acc_valid'] = smooth_acc_valid\n",
        "analysis_arguments['smooth_acc_target'] = smooth_acc_target\n",
        "analysis_arguments['smooth_moments'] = smooth_moments\n",
        "analysis_arguments['multiply_score_by_interval'] = multiply_score_by_interval\n",
        "analysis_arguments['stop_at_first_maximum'] = stop_at_first_maximum\n",
        "analysis_arguments['num_smoothing_steps'] = num_smoothing_steps\n",
        "analysis_arguments['stopping_criterion'] = stopping_criterion\n",
        "\n",
        "analysis_arguments['critical_layer_criterion'] = critical_layer_criterion\n",
        "analysis_arguments['critical_moment_criterion'] = critical_moment_criterion\n",
        "analysis_arguments['critical_time_criterion'] = critical_time_criterion"
      ],
      "metadata": {
        "id": "_sOoYtE01vAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "performance_dict = {}\n",
        "start = time.time()\n",
        "for algo in algos:\n",
        "    print('*********************************************************')\n",
        "    print('ALGO : ' + algo)\n",
        "    performance_dict[algo] = {}\n",
        "    for source_dataset in source_datasets:\n",
        "        print('---------------------------------------------------------')\n",
        "        print('SOURCE : ' + source_dataset)\n",
        "        print('---------------------------------------------------------')\n",
        "        run = training_runs_dict[algo][source_dataset]\n",
        "        if run is None:\n",
        "            print('Model not trained yet.')\n",
        "        else:\n",
        "            # Loading train accuracy\n",
        "            try:\n",
        "                results_train = np.loadtxt(exp_dir_train_acc + 'run-' + algo + '_' + source_dataset + '_run-' + str(run) + '-tag-train_acc.csv', delimiter=',', skiprows=1)\n",
        "                acc_train = results_train[:, 2]\n",
        "                if smooth_acc_train: \n",
        "                    acc_train = temporal_smoothing_v3(acc_train, step=num_smoothing_steps)\n",
        "                iters_train = results_train[:, 1].astype(int)\n",
        "            except:\n",
        "                print('Could not load train accuracy')\n",
        "            # Loading valid accuracy\n",
        "            try:\n",
        "                results_valid = np.loadtxt(exp_dir_valid_acc + 'run-' + algo + '_' + source_dataset + '_run-' + str(\n",
        "                    run) + '-tag-mean valid acc.csv', delimiter=',', skiprows=1)\n",
        "                acc_valid = results_valid[:, 2]\n",
        "                if smooth_acc_valid: \n",
        "                    acc_valid = temporal_smoothing_v3(acc_valid, step=num_smoothing_steps)\n",
        "                iters_valid = results_valid[:, 1].astype(int)\n",
        "            except:\n",
        "                print('Could not load valid accuracy')\n",
        "                continue\n",
        "            # Plotting the training and validation accuracy\n",
        "            if plot_accuracy:\n",
        "                try:\n",
        "                    plot_acc_train_valid(acc_train, iters_train, acc_valid, iters_valid)\n",
        "                except:\n",
        "                    print('Train or Valid accuracy is missing.')\n",
        "            # Loading source activation moments\n",
        "            try:\n",
        "                dataset_valid_moments = source_dataset\n",
        "\n",
        "                moments_source, moments_source_iters = get_moments(\n",
        "                    exp_dir_activation_trajectory + algo + '_' + source_dataset + '_run-' + str(\n",
        "                        run) + '_activation_moments_trajectory_analysis_' + dataset_valid_moments + '/activation_moments_trajectory.npy')\n",
        "\n",
        "                # Averaging source moments across all tasks\n",
        "                moments_source = np.mean(moments_source, axis=2)\n",
        "                \n",
        "                if smooth_moments:\n",
        "                    for i in range(moments_source.shape[0]):\n",
        "                        for j in range(moments_source.shape[2]):\n",
        "                            moments_source[i, :, j] = temporal_smoothing_v3(moments_source[i, :, j], step=num_smoothing_steps)\n",
        "\n",
        "            except:\n",
        "                print('Could not load the source activation moments')\n",
        "                continue\n",
        "\n",
        "            performance_dict[algo][source_dataset] = {}\n",
        "            # Looping over all target datasets\n",
        "            for target_dataset in target_datasets:\n",
        "                # Loading target accuracy\n",
        "                acc_target, iters_target = load_target_acc(exp_dir_target_acc, algo, source_dataset, run,\n",
        "                                                           target_dataset)\n",
        "                \n",
        "                acc_target = np.mean(acc_target, axis=1)\n",
        "                if smooth_acc_target:\n",
        "                    acc_target = temporal_smoothing_v3(acc_target, step=num_smoothing_steps)\n",
        "                \n",
        "                if (acc_target is None) or (iters_target is None):\n",
        "                    print('Could not load the target accuracy.')\n",
        "                    continue\n",
        "                          \n",
        "                if plot_accuracy and (acc_target is not None) and (iters_target is not None):\n",
        "                    plot_acc_target(acc_target, iters_target, target_dataset)\n",
        "                \n",
        "                # Loading target activation moments trajectory\n",
        "                try:\n",
        "                    moments_target, moments_target_iters = get_moments(\n",
        "                        exp_dir_activation_trajectory + algo + '_' + source_dataset + '_run-' + str(\n",
        "                            run) + '_activation_moments_trajectory_analysis_' + target_dataset + '/activation_moments_trajectory.npy')\n",
        "                except:\n",
        "                    print('Could not load the target activation moments.')\n",
        "                    continue\n",
        "                    \n",
        "                if smooth_moments:\n",
        "                    for i in range(moments_target.shape[0]):\n",
        "                        for j in range(moments_target.shape[2]):\n",
        "                            for k in range(moments_target.shape[3]):\n",
        "                                moments_target[i, :, j, k] = temporal_smoothing_v3(moments_target[i, :, j, k], step=num_smoothing_steps)\n",
        "                    \n",
        "                if plot_moments:\n",
        "                    plot_activation_moments(moments_source, moments_source_iters, moments_target, moments_target_iters,\n",
        "                                            acc_valid, layers, show_corr, task_id)\n",
        "                # Check if moment trajectories have more than one point\n",
        "                if moments_target.shape[1] <= 1 or moments_source.shape[1] <= 1:\n",
        "                    print('Moment trajectories less than two points. Cannot evaluate method.')\n",
        "                    continue\n",
        "\n",
        "                # EVALUATING THE METHOD PERFORMANCE\n",
        "                performance_dict[algo][source_dataset][target_dataset] = {}\n",
        "\n",
        "                if stopping_criterion == 'negative_correlation_before_max_valid':\n",
        "                    perf_avg, perf_valid, critical_layer_idx, critical_moment_idx, list_perf, list_iter_stopping = negative_correlation_before_max_valid(moments_target, moments_target_iters, moments_source, acc_valid, iters_valid, acc_target, iters_target, show_stopping=show_stopping, multiply_score_by_interval=multiply_score_by_interval, stop_at_first_maximum=stop_at_first_maximum, force_layer_moment_inspection=force_layer_moment_inspection,\n",
        "                                                                                                                          critical_layer_criterion=critical_layer_criterion,\n",
        "                                                                                                                          critical_moment_criterion=critical_moment_criterion,\n",
        "                                                                                                                          critical_time_criterion=critical_time_criterion)\n",
        "                    performance_dict[algo][source_dataset][target_dataset]['critical_layer_idx'] = critical_layer_idx\n",
        "                    performance_dict[algo][source_dataset][target_dataset]['critical_moment_idx'] = critical_moment_idx\n",
        "                    \n",
        "                    performance_dict[algo][source_dataset][target_dataset]['list_perf'] = list_perf\n",
        "                    performance_dict[algo][source_dataset][target_dataset]['list_iter_stopping'] = list_iter_stopping\n",
        "\n",
        "                else:\n",
        "                    print('Invalid stopping criterion ! Options :\\n[decorrelation_criterion,\\ndistributional_shift_inflextion,\\ndecorrelation_before_max_valid,\\nnegative_correlation_averaged_before_max_valid]')\n",
        "                    sys.exit()\n",
        "                \n",
        "                # Saving the performances in the dictionary\n",
        "                performance_dict[algo][source_dataset][target_dataset]['our_method'] = perf_avg\n",
        "                performance_dict[algo][source_dataset][target_dataset]['baseline_validation'] = perf_valid\n",
        "                performance_dict[algo][source_dataset][target_dataset]['acc_target_max'] = np.max(acc_target)\n",
        "                performance_dict[algo][source_dataset][target_dataset]['acc_target_min'] = np.min(acc_target)            \n",
        "                \n",
        "print('\\n\\n\\n###################################################')\n",
        "print('ANALYSIS COMPLETE !')\n",
        "end = time.time()\n",
        "print('Running time : ' + str(end - start))"
      ],
      "metadata": {
        "id": "EILOXO432LCS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}